{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b8cc3-7527-4606-a209-56e83aee9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import os\n",
    "from pydicom.pixel_data_handlers.util import convert_color_space\n",
    "\n",
    "def dicom_to_matrix_recursive(dicom_folder_path, output_folder_path):\n",
    "    \"\"\"\n",
    "    Recursively converts all DICOM files in a specified folder (and its subfolders) to numpy matrices and saves them as .npy files.\n",
    "\n",
    "    Parameters:\n",
    "    dicom_folder_path (str): Path to the folder containing DICOM files.\n",
    "    output_folder_path (str): Path to the folder where .npy files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    for root, dirs, files in os.walk(dicom_folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".dcm\"):  # Check if the file is a DICOM file\n",
    "                dicom_path = os.path.join(root, filename)\n",
    "\n",
    "                try:\n",
    "                \n",
    "                    dicom_data = pydicom.dcmread(dicom_path)\n",
    "\n",
    "                    if dicom_data.file_meta.TransferSyntaxUID.is_compressed:\n",
    "                        dicom_data.decompress()\n",
    "\n",
    "                    pixel_array = dicom_data.pixel_array\n",
    "\n",
    "                    pixel_matrix = np.array(pixel_array)\n",
    "\n",
    "                    relative_path = os.path.relpath(root, dicom_folder_path)\n",
    "                    output_dir = os.path.join(output_folder_path, relative_path)\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "\n",
    "                    output_file_path = os.path.join(output_dir, filename.replace(\".dcm\", \".npy\"))\n",
    "                    np.save(output_file_path, pixel_matrix)\n",
    "\n",
    "                    print(f\"Converted {dicom_path} to {output_file_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {dicom_path}: {e}\")\n",
    "\n",
    "dicom_folder_path = 'C:/Users/BUSRA/Downloads/teknofest/demo_dataset'\n",
    "output_folder_path = 'C:/Users/BUSRA/Documents/Teknofest-2024/npyverilerFİNAL'\n",
    "dicom_to_matrix_recursive(dicom_folder_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0136c2a9-b534-4cab-9d56-2546b5bec955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama PSNR: 26.45\n",
      "Ortalama SSIM: 0.8833\n",
      "Ortalama MSE: 172.60\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "\n",
    "def gaussian_bulaniklastirma(image, kernel_boyutu=(5, 5)):\n",
    "    return cv2.GaussianBlur(image, kernel_boyutu, 0)\n",
    "\n",
    "def clahe_uygula(image, clip_limit=1.0, tile_grid_size=(8, 8)):\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    return clahe.apply(image)\n",
    "\n",
    "def normalize_et(image):\n",
    "    normalized_image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    return normalized_image.astype(np.uint8)\n",
    "\n",
    "def goruntu_kalitesini_degerlendir(orijinal, islenmis):\n",
    "    psnr_degeri = psnr(orijinal, islenmis)\n",
    "    ssim_degeri = ssim(orijinal, islenmis)\n",
    "    mse_degeri = mse(orijinal, islenmis)\n",
    "    return psnr_degeri, ssim_degeri, mse_degeri\n",
    "\n",
    "def roi_cikar(image):\n",
    "    norm_image = normalize_et(image)\n",
    "    thresholded_image = cv2.adaptiveThreshold(norm_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                              cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    temizlenmis_goruntu = cv2.morphologyEx(thresholded_image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(temizlenmis_goruntu, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        en_buyuk_kontur = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(en_buyuk_kontur)\n",
    "        roi_image = image[y:y+h, x:x+w]\n",
    "        return roi_image\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "npy_directory = 'C:/Users/BUSRA/Documents/Teknofest-2024/npyverilerFİNAL'\n",
    "\n",
    "ana_cikti_klasoru = 'C:/Users/BUSRA/Documents/Teknofest-2024/Islenmis_GoruntulerFİNAL'\n",
    "\n",
    "psnr_listesi = []\n",
    "ssim_listesi = []\n",
    "mse_listesi = []\n",
    "\n",
    "for root, dirs, files in os.walk(npy_directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):\n",
    "            dosya_yolu = os.path.join(root, file)\n",
    "            \n",
    "            try:\n",
    "                orijinal_goruntu = np.load(dosya_yolu)\n",
    "\n",
    "                # Extract ROI\n",
    "                roi_goruntu = roi_cikar(orijinal_goruntu)\n",
    "\n",
    "                # Normalize the ROI\n",
    "                roi_goruntu = normalize_et(roi_goruntu)\n",
    "\n",
    "                # Noise Reduction: Gaussian Blur\n",
    "                bulanık_goruntu = gaussian_bulaniklastirma(roi_goruntu)\n",
    "\n",
    "                # Contrast Enhancement: CLAHE\n",
    "                clahe_goruntu = clahe_uygula(bulanık_goruntu, clip_limit=1.0, tile_grid_size=(8, 8))\n",
    "\n",
    "                # Normalization: After CLAHE\n",
    "                son_goruntu = normalize_et(clahe_goruntu)\n",
    "\n",
    "                psnr_degeri, ssim_degeri, mse_degeri = goruntu_kalitesini_degerlendir(roi_goruntu, son_goruntu)\n",
    "                psnr_listesi.append(psnr_degeri)\n",
    "                ssim_listesi.append(ssim_degeri)\n",
    "                mse_listesi.append(mse_degeri)\n",
    "\n",
    "                yeni_dosya_yolu = dosya_yolu.replace('npyveriler', 'Islenmis_Goruntuler').replace('.npy', '_processed.npy')\n",
    "                os.makedirs(os.path.dirname(yeni_dosya_yolu), exist_ok=True)\n",
    "\n",
    "                np.save(yeni_dosya_yolu, son_goruntu)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{dosya_yolu} dosyası işlenirken hata oluştu: {e}\")\n",
    "\n",
    "# Calculate and print average metric values\n",
    "ortalama_psnr = np.mean(psnr_listesi)\n",
    "ortalama_ssim = np.mean(ssim_listesi)\n",
    "ortalama_mse = np.mean(mse_listesi)\n",
    "\n",
    "print(f\"Ortalama PSNR: {ortalama_psnr:.2f}\")\n",
    "print(f\"Ortalama SSIM: {ortalama_ssim:.4f}\")\n",
    "print(f\"Ortalama MSE: {ortalama_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67da479-ae08-4c2a-9cc0-f76c5ffc27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# İşlenmiş görüntülerin bulunduğu dizin\n",
    "islenmis_goruntu_klasoru = 'C:/Users/BUSRA/Documents/Teknofest-2024/Islenmis_Goruntuler'\n",
    "\n",
    "# Görüntüleme yapılacak görüntü sayısı\n",
    "gosterilecek_goruntu_sayisi = 20\n",
    "\n",
    "# Tüm .npy dosyalarını bulma\n",
    "islenmis_dosyalar = []\n",
    "for root, dirs, files in os.walk(islenmis_goruntu_klasoru):\n",
    "    for file in files:\n",
    "        if file.endswith('_processed.npy'):\n",
    "            islenmis_dosyalar.append(os.path.join(root, file))\n",
    "\n",
    "# Eğer dosya yoksa çıkış yap\n",
    "if not islenmis_dosyalar:\n",
    "    print(\"İşlenmiş dosya bulunamadı.\")\n",
    "else:\n",
    "    # Rastgele dosyaları seçme\n",
    "    secilen_dosyalar = random.sample(islenmis_dosyalar, min(gosterilecek_goruntu_sayisi, len(islenmis_dosyalar)))\n",
    "\n",
    "    # Seçilen dosyaları görüntüleme\n",
    "    for dosya_yolu in secilen_dosyalar:\n",
    "        # .npy dosyasını yükle\n",
    "        goruntu = np.load(dosya_yolu)\n",
    "        \n",
    "        # Görüntüyü göster\n",
    "        plt.figure()\n",
    "        plt.imshow(goruntu, cmap='gray')\n",
    "        plt.title(f'Görüntü: {os.path.basename(dosya_yolu)}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Görüntüleri göster\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581af5d-4ffe-4d57-937e-3b06442dec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage import exposure\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasının yolunu belirtin\n",
    "csv_file_path = 'C:/Users/BUSRA/Documents/Teknofest-2024/Dataframe_NPY.csv'  # Kendi CSV dosyanızın yolunu belirtin\n",
    "\n",
    "# CSV dosyasını oku\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Custom Dataset Sınıfı\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, apply_clahe=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.apply_clahe = apply_clahe\n",
    "        self.missing_files = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['NPY DOSYA YOLU']\n",
    "        label = self.dataframe.iloc[idx]['birads']\n",
    "\n",
    "        label = 1 if label >= 2 else 0\n",
    "\n",
    "        try:\n",
    "            img_data = np.load(img_path)\n",
    "            img_data = Image.fromarray(img_data)\n",
    "            \n",
    "            if self.apply_clahe:\n",
    "                img_data = np.array(img_data)\n",
    "                img_data = exposure.equalize_adapthist(img_data)  # CLAHE uygulama\n",
    "                img_data = Image.fromarray((img_data * 255).astype(np.uint8))\n",
    "\n",
    "            if img_data.mode != 'L':\n",
    "                img_data = img_data.convert('L')\n",
    "\n",
    "            if self.transform:\n",
    "                img_data = self.transform(img_data)\n",
    "\n",
    "            return img_data, label\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self.missing_files.append(img_path)\n",
    "            print(f\"Dosya bulunamadı: {img_path}\")\n",
    "            return None, None\n",
    "\n",
    "# Özelleştirilmiş collate_fn fonksiyonu\n",
    "def custom_collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0)\n",
    "    \n",
    "    images, labels = zip(*batch)\n",
    "    return torch.stack(images, 0), torch.tensor(labels)\n",
    "\n",
    "# Veriyi Eğitim, Doğrulama ve Test Setlerine Bölmek\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Veri Dönüşümleri\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # ResNet ve AlexNet RGB bekler\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset ve DataLoader Oluşturma\n",
    "train_dataset = MammographyDataset(dataframe=train_df, transform=transform, apply_clahe=True)\n",
    "valid_dataset = MammographyDataset(dataframe=valid_df, transform=transform)\n",
    "test_dataset = MammographyDataset(dataframe=test_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# AlexNet ve ResNet50 Ensemble Modeli\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # ResNet50 modeli\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()  # Son sınıflandırma katmanını kaldır\n",
    "        \n",
    "        # AlexNet modeli\n",
    "        self.alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "        self.alexnet.classifier[6] = nn.Identity()  # Son sınıflandırma katmanını kaldır\n",
    "        \n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(2048 + 4096, 512),  # Birleştirilmiş özelliklerin giriş boyutunu düzelt\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.resnet(x)  \n",
    "        x2 = self.alexnet(x)  \n",
    "\n",
    "        x_combined = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        x_out = self.dnn(x_combined)\n",
    "        return x_out\n",
    "\n",
    "model = EnsembleModel()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                outputs = model(images)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Model forward geçiş hatası: {e}\")\n",
    "                print(f\"Girdi boyutları: {images.shape}\")\n",
    "                continue\n",
    "\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            \n",
    "            try:\n",
    "                loss.backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Loss backward geçiş hatası: {e}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "           \n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, test_preds)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=70)\n",
    "\n",
    "validate_model(model, valid_loader)\n",
    "\n",
    "test_model(model, test_loader)\n",
    "\n",
    "if train_dataset.missing_files:\n",
    "    print(f\"Toplam eksik dosya sayısı: {len(train_dataset.missing_files)}\")\n",
    "    print(\"Eksik dosyalar:\")\n",
    "    for missing_file in train_dataset.missing_files:\n",
    "        print(missing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94e66b-0add-419a-9afc-2da79fd8c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage import exposure\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_file_path = 'C:/Users/BUSRA/Documents/Teknofest-2024/Dataframe_NPY.csv'  # Kendi CSV dosyanızın yolunu belirtin\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, apply_clahe=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.apply_clahe = apply_clahe\n",
    "        self.missing_files = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['NPY DOSYA YOLU']\n",
    "        birads_label = self.dataframe.iloc[idx]['birads']\n",
    "        etiket_adi = self.dataframe.iloc[idx]['ETİKET ADI']\n",
    "\n",
    "     \n",
    "        birads_label = 1 if birads_label >= 2 else 0\n",
    "        \n",
    "       \n",
    "        etiket_mapping = {'Kitle': 0, 'Kalsifikasyon': 1, 'Normal': 2}\n",
    "        etiket_label = etiket_mapping[etiket_adi]\n",
    "\n",
    "        try:\n",
    "            img_data = np.load(img_path)\n",
    "            img_data = Image.fromarray(img_data)\n",
    "            \n",
    "            if self.apply_clahe:\n",
    "                img_data = np.array(img_data)\n",
    "                img_data = exposure.equalize_adapthist(img_data)  # CLAHE uygulama\n",
    "                img_data = Image.fromarray((img_data * 255).astype(np.uint8))\n",
    "\n",
    "            if img_data.mode != 'L':\n",
    "                img_data = img_data.convert('L')\n",
    "\n",
    "            if self.transform:\n",
    "                img_data = self.transform(img_data)\n",
    "\n",
    "            return img_data, (birads_label, etiket_label)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self.missing_files.append(img_path)\n",
    "            print(f\"Dosya bulunamadı: {img_path}\")\n",
    "            return None, (None, None)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), (torch.empty(0), torch.empty(0))\n",
    "    \n",
    "    images, labels = zip(*batch)\n",
    "    birads_labels, etiket_labels = zip(*labels)\n",
    "    return torch.stack(images, 0), (torch.tensor(birads_labels), torch.tensor(etiket_labels))\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # ResNet ve AlexNet RGB bekler\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = MammographyDataset(dataframe=train_df, transform=transform, apply_clahe=True)\n",
    "valid_dataset = MammographyDataset(dataframe=valid_df, transform=transform)\n",
    "test_dataset = MammographyDataset(dataframe=test_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # ResNet50 modeli\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()  # Son sınıflandırma katmanını kaldır\n",
    "        \n",
    "        # AlexNet modeli\n",
    "        self.alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "        self.alexnet.classifier[6] = nn.Identity()  # Son sınıflandırma katmanını kaldır\n",
    "        \n",
    "        # DNN katmanı, doğru çıktı boyutlarına göre ayarlandı\n",
    "        self.dnn_birads = nn.Sequential(\n",
    "            nn.Linear(2048 + 4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1),  # Binary classification output for birads\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.dnn_etiket = nn.Sequential(\n",
    "            nn.Linear(2048 + 4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 3), \n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.resnet(x)  # ResNet50 çıktısı\n",
    "        x2 = self.alexnet(x)  # AlexNet çıktısı\n",
    "\n",
    "        # Her iki modelden gelen özellikleri birleştir\n",
    "        x_combined = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Ayrı çıkışlar\n",
    "        birads_out = self.dnn_birads(x_combined)\n",
    "        etiket_out = self.dnn_etiket(x_combined)\n",
    "        return birads_out, etiket_out\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, filename='checkpoint.pth.tar'):\n",
    "    state = {'epoch': epoch,\n",
    "             'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict': optimizer.state_dict()}\n",
    "    torch.save(state, filename)\n",
    "\n",
    "model = EnsembleModel()\n",
    "criterion_birads = nn.BCELoss()\n",
    "criterion_etiket = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "checkpoint_path = 'checkpoint.pth.tar'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model, optimizer, start_epoch = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "    print(f\"Checkpoint yükleniyor - Eğitim {start_epoch}. epoch'tan devam edecek.\")\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion_birads, criterion_etiket, optimizer, epochs=20, start_epoch=0):\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        running_loss_birads = 0.0\n",
    "        running_loss_etiket = 0.0\n",
    "        all_preds_birads = []\n",
    "        all_labels_birads = []\n",
    "        all_preds_etiket = []\n",
    "        all_labels_etiket = []\n",
    "        \n",
    "        for images, (birads_labels, etiket_labels) in train_loader:\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "           \n",
    "            if torch.cuda.is_available():\n",
    "                images, birads_labels, etiket_labels = images.cuda(), birads_labels.cuda(), etiket_labels.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                outputs_birads, outputs_etiket = model(images)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Model forward geçiş hatası: {e}\")\n",
    "                print(f\"Girdi boyutları: {images.shape}\")\n",
    "                continue\n",
    "\n",
    "            loss_birads = criterion_birads(outputs_birads, birads_labels.unsqueeze(1).float())\n",
    "            loss_etiket = criterion_etiket(outputs_etiket, etiket_labels)\n",
    "            total_loss = loss_birads + loss_etiket\n",
    "            \n",
    "            try:\n",
    "                total_loss.backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Loss backward geçiş hatası: {e}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss_birads += loss_birads.item()\n",
    "            running_loss_etiket += loss_etiket.item()\n",
    "\n",
    "        \n",
    "            all_preds_birads.extend((outputs_birads > 0.5).cpu().numpy())\n",
    "            all_labels_birads.extend(birads_labels.cpu().numpy())\n",
    "            \n",
    "            _, preds_etiket = torch.max(outputs_etiket, 1)\n",
    "            all_preds_etiket.extend(preds_etiket.cpu().numpy())\n",
    "            all_labels_etiket.extend(etiket_labels.cpu().numpy())\n",
    "\n",
    "        accuracy_birads = accuracy_score(all_labels_birads, all_preds_birads)\n",
    "        accuracy_etiket = accuracy_score(all_labels_etiket, all_preds_etiket)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss Birads: {running_loss_birads/len(train_loader)}, Loss Etiket: {running_loss_etiket/len(train_loader)}, Accuracy Birads: {accuracy_birads:.4f}, Accuracy Etiket: {accuracy_etiket:.4f}')\n",
    "        \n",
    "      \n",
    "        save_checkpoint(epoch+1, model, optimizer, filename=checkpoint_path)\n",
    "\n",
    "# Doğrulama Fonksiyonu\n",
    "def validate_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    val_preds_birads = []\n",
    "    val_labels_birads = []\n",
    "    val_preds_etiket = []\n",
    "    val_labels_etiket = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, (birads_labels, etiket_labels) in valid_loader:\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "           \n",
    "            if torch.cuda.is_available():\n",
    "                images, birads_labels, etiket_labels = images.cuda(), birads_labels.cuda(), etiket_labels.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            outputs_birads, outputs_etiket = model(images)\n",
    "            \n",
    "            val_preds_birads.extend((outputs_birads > 0.5).cpu().numpy())\n",
    "            val_labels_birads.extend(birads_labels.cpu().numpy())\n",
    "            \n",
    "            _, preds_etiket = torch.max(outputs_etiket, 1)\n",
    "            val_preds_etiket.extend(preds_etiket.cpu().numpy())\n",
    "            val_labels_etiket.extend(etiket_labels.cpu().numpy())\n",
    "\n",
    "    accuracy_birads = accuracy_score(val_labels_birads, val_preds_birads)\n",
    "    accuracy_etiket = accuracy_score(val_labels_etiket, val_preds_etiket)\n",
    "    print(f'Validation Accuracy Birads: {accuracy_birads:.4f}, Validation Accuracy Etiket: {accuracy_etiket:.4f}')\n",
    "\n",
    "# Test Fonksiyonu\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_preds_birads = []\n",
    "    test_labels_birads = []\n",
    "    test_preds_etiket = []\n",
    "    test_labels_etiket = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, (birads_labels, etiket_labels) in test_loader:\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                images, birads_labels, etiket_labels = images.cuda(), birads_labels.cuda(), etiket_labels.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            outputs_birads, outputs_etiket = model(images)\n",
    "            \n",
    "            test_preds_birads.extend((outputs_birads > 0.5).cpu().numpy())\n",
    "            test_labels_birads.extend(birads_labels.cpu().numpy())\n",
    "            \n",
    "            _, preds_etiket = torch.max(outputs_etiket, 1)\n",
    "            test_preds_etiket.extend(preds_etiket.cpu().numpy())\n",
    "            test_labels_etiket.extend(etiket_labels.cpu().numpy())\n",
    "\n",
    "    accuracy_birads = accuracy_score(test_labels_birads, test_preds_birads)\n",
    "    accuracy_etiket = accuracy_score(test_labels_etiket, test_preds_etiket)\n",
    "    print(f'Test Accuracy Birads: {accuracy_birads:.4f}, Test Accuracy Etiket: {accuracy_etiket:.4f}')\n",
    "\n",
    "train_model(model, train_loader, criterion_birads, criterion_etiket, optimizer, epochs=20, start_epoch=start_epoch)\n",
    "\n",
    "\n",
    "validate_model(model, valid_loader)\n",
    "\n",
    "\n",
    "test_model(model, test_loader)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'ensemble_model.pth')\n",
    "\n",
    "\n",
    "if train_dataset.missing_files:\n",
    "    print(f\"Toplam eksik dosya sayısı: {len(train_dataset.missing_files)}\")\n",
    "    print(\"Eksik dosyalar:\")\n",
    "    for missing_file in train_dataset.missing_files:\n",
    "        print(missing_file)\n",
    "\n",
    "train_model(model, train_loader, criterion_birads, criterion_etiket, optimizer, epochs=20, start_epoch=start_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9f87bb1-d2ab-41a3-8bd0-b2554a384b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import json\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # ResNet50 modeli\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity() \n",
    "        \n",
    "        # AlexNet modeli\n",
    "        self.alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "        self.alexnet.classifier[6] = nn.Identity() \n",
    "        \n",
    "       \n",
    "        self.dnn_birads = nn.Sequential(\n",
    "            nn.Linear(2048 + 4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.dnn_etiket = nn.Sequential(\n",
    "            nn.Linear(2048 + 4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 3),  \n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.resnet(x)  \n",
    "        x2 = self.alexnet(x)  \n",
    "\n",
    "     \n",
    "        x_combined = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Ayrı çıkışlar\n",
    "        birads_out = self.dnn_birads(x_combined)\n",
    "        etiket_out = self.dnn_etiket(x_combined)\n",
    "        return birads_out, etiket_out\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, model):\n",
    "    checkpoint = torch.load(filepath, weights_only=True) \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = EnsembleModel()\n",
    "model = load_checkpoint(checkpoint_path, model)\n",
    "model.eval()  \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for images, image_names in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "\n",
    "            outputs_birads, outputs_etiket = model(images)\n",
    "            \n",
    "        \n",
    "            for i, name in enumerate(image_names):\n",
    "                birads_confidence = outputs_birads[i].item()\n",
    "                etiket_label = torch.argmax(outputs_etiket[i]).item()\n",
    "                etiket_confidence = outputs_etiket[i].max().item()\n",
    "                \n",
    "                predictions[name] = {\n",
    "                    \"birads_confidence\": birads_confidence,\n",
    "                    \"etiket_label\": etiket_label,\n",
    "                    \"etiket_confidence\": etiket_confidence\n",
    "                }\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def save_predictions_to_json(predictions, output_file='tahminler.json'):\n",
    "    output_data = {\n",
    "        \"kunye\": {\n",
    "            \"takim_adi\": \"DIAGNOS-AI\",  # Takım adı\n",
    "            \"takim_id\": \"344193\",        # Takım ID\n",
    "            \"aciklama\": \"Model tahmin sonuçları\",\n",
    "            \"versiyon\": \"1.0\"\n",
    "        },\n",
    "        \"tahminler\": {}\n",
    "    }\n",
    "\n",
    "    for name, pred in predictions.items():\n",
    "        output_data[\"tahminler\"][name] = {\n",
    "            \"kategori\": str(pred[\"etiket_label\"]),\n",
    "            \"dosya\": {\n",
    "                name: {\n",
    "                    \"kitle\": [\n",
    "                        {\"kutu\": \"0,0;1,1;1,0;0,1\", \"guven\": str(pred[\"birads_confidence\"])}  # Örnek kutu değerleri\n",
    "                    ],\n",
    "                    \"kalsifikasyon\": []  \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(output_data, json_file, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6e8d0-cf74-4381-a196-d05397fefe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Dataset sınıfı tanımı\n",
    "\n",
    "\n",
    "# Görüntülerin bulunduğu ana dizin\n",
    "root_dir = 'C:/Users/BUSRA/Documents/Teknofest-2024/FİNAL'\n",
    "\n",
    "\n",
    "\n",
    "# Görselleri modelden geçirme\n",
    "for idx in range(len(test_dataset)):\n",
    "    image, image_path = test_dataset[idx] \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image = image.cuda()\n",
    "\n",
    "    outputs_birads, outputs_etiket = model(image.unsqueeze(0)) \n",
    "    \n",
    "    print(f\"Görüntü: {image_path}\")\n",
    "    print(\"Birads Output:\", outputs_birads)        \n",
    "    print(\"Etiket Output:\", outputs_etiket)         \n",
    "    break \n",
    "\n",
    "\n",
    "# BIRADS ve Etiket tahminlerini işlemeyi gösteren örnek\n",
    "for images, image_names in test_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "\n",
    "    # Modelden tahminleri al\n",
    "    outputs_birads, outputs_etiket = model(images)\n",
    "\n",
    "    birads_pred = (outputs_birads > 0.5).float()  # 0.5 üstü pozitif, altı negatif\n",
    "    print(\"BIRADS Tahminleri:\", birads_pred)\n",
    "\n",
    "    # Etiketler için en yüksek olasılığı bulma\n",
    "    etiket_pred = torch.argmax(outputs_etiket, dim=1)  \n",
    "    print(\"Etiket Tahminleri:\", etiket_pred)\n",
    "\n",
    "    break  # Sadece bir batch test etmek için\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee13fe0-510f-488b-97a5-9465abb6c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for images, image_names in test_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "\n",
    "   \n",
    "    outputs_birads, outputs_etiket = model(images)\n",
    "\n",
    "    # BIRADS için 0.5 eşik değeri kullanarak sınıflandırma\n",
    "    birads_pred = (outputs_birads > 0.5).float()  # 0.5 üstü pozitif, altı negatif\n",
    "    print(\"BIRADS Tahminleri:\", birads_pred)\n",
    "\n",
    "    # Etiketler için en yüksek olasılığı bulma\n",
    "    etiket_pred = torch.argmax(outputs_etiket, dim=1)  # En yüksek olasılık hangi sınıfta?\n",
    "    print(\"Etiket Tahminleri:\", etiket_pred)\n",
    "\n",
    "    break  # Sadece bir batch test etmek için\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b79f44cb-c7a4-4a5c-8403-e07a1e0186c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.17.0\n",
      "GPU Device Available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"GPU Device Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab6643d2-bdda-482d-88b6-0ee6fd0a2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya bulundu!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "csv_file = 'C:/Users/BUSRA/Documents/Teknofest-2024/Dataframe_NPY.csv'\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    print(\"Dosya bulundu!\")\n",
    "else:\n",
    "    print(\"Dosya bulunamadı. Lütfen dosya yolunu kontrol edin.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c80d6-1403-4941-9138-3b49dfed6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch versiyonu:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303aa487-8408-4c9e-b98e-f7ccd2b3b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow versiyonu:\", tf.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
